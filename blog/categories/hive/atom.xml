<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hive | An Imbalist]]></title>
  <link href="http://Imbalism.github.com/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://Imbalism.github.com/"/>
  <updated>2011-12-20T11:03:21+08:00</updated>
  <id>http://Imbalism.github.com/</id>
  <author>
    <name><![CDATA[Yunxing dai]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Better Map join in Hive]]></title>
    <link href="http://Imbalism.github.com/blog/2011/12/11/map-join-in-hive/"/>
    <updated>2011-12-11T01:47:00+08:00</updated>
    <id>http://Imbalism.github.com/blog/2011/12/11/map-join-in-hive</id>
    <content type="html"><![CDATA[<p>Although <a href="hadoop.apache.org" title="Hadoop">Hadoop</a> provides a general way to handle distributed data, it is hard to write map reduce program every time you want to handle some data. At this time, <a href="hive.apache.org">Hive</a> comes into play to save people from the hell of writing map reduce programs.</p>

<p><img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="Hadoop" /> <img src="http://hive.apache.org/images/hive_logo_medium.jpg" alt="Hive" /></p>

<p>Hive is a data warehouse built on top of Hadoop. It provides a SQL-like interface so that SQL programmer can easily pick it up. Data warehouse means the operation for this database should be read-optimized. Instead of thousands of transactions at a time, there are not so many request to Hive, and most of these request should be query and bulk loading.</p>

<p>Join is an important part in query. How to proceed join fast is extremely important in this read-optimization scenario. An fairly straight forward way, which is called <strong>Common Join</strong>, to join in map-reduce environment is divided into 3 phases:</p>

<ul>
<li> <strong>map phase</strong> which split and read the whole table and emit key(join key) value(select value) pairs</li>
<li> <strong>shuffle phase</strong> which merges and sorts these pairs by key</li>
<li> <strong>reduce phase</strong> which does the actual join work by combining all the pairs with the same key together</li>
</ul>


<p>Amoung these three phases, shuffle phase is most time consuming. If a table is small enough to fit into mapper's memory after it is converted into <em>hashtable</em>(with the same key value pair as mentioned above), then <strong>Map Join</strong> is introduced, which builds a hashtable file for the small table, copy the file to each mapper so that each mapper can probe a split of the bigtable and get the join result. By doing this way, the system can avoid shuffle and reduce phase, result in significant speed-up.</p>

<p>Some points have to be clear</p>

<ol>
<li>A table in Hive is actually a file in file system.</li>
<li>In map join, this relational table on disk is converted to a hashtable in memory.</li>
<li>A small table file on disk could be too large to fit into memory for two reasons, the first one is it may be compressed, the other is that the key density could be high(a low key density means most keys are same in the relational table, after it turns into hashtable, all the pairs with the same key can be stored using just one key.).</li>
</ol>


<p>So here comes the question, how can we know if a table can fit into memory after it is converted into hashtable? Current solution of Hive is somewhat conservative, it just read the size of the table file. If it is larger than 25MB, Hive thinks it would be too large and do common join. If it is smaller than 25MB, Hive will do map join instead because it thinks this table can fit into memory.</p>

<p>The above solution is somehow unreasonable because as we mentioned before, table file size has no so strong connect to hashtable size.</p>

<p>Our course project in [EECS 584, Advanced Database System] aims at proposing a better selecting criteria for map join and common join.</p>

<p>Instead of trying to infer hashtable size at runtime, our solution, in short, tries to get the exactly hashtable size when loading data into the table. That is, we build a hash table beforehand, when we loads data into the table. To reduce the amount of hashtables (Cound be 2<sup>N</sup> tables if we want to cover all the fields as potential join keys), we propose a way similar as building an index(we select interesting fields and maintain an index, instead of building indexes for every column combinations). A new keyword is proposed for this purpose, "<strong>PRECOMPUTE</strong>". The usage of this keyword goes like this:</p>

<pre><code>CREATE TABLE People(id STRING, name String, age INT) PRECOMPUTE id;
</code></pre>

<p>This will build a hashtable in advance in the memory. Note that <strong>once the hashtable exceeds the memory limits, the building process will automatically break and label this field as 'bad for hash'.</strong></p>

<p>Another advantage of this is that, consider the following query, :</p>

<pre><code>Select * FROM People p Where p.age = '0000001';
</code></pre>

<p>The query will build a hashtable for small table whose size equals 1 (because of the single selection predicate). This will definitely fits in the memory. However, in previous implementation, if the file size exceeds 25MBs (or whatever defined by user), Hive will use Common Join to execute it, although it can use mapjoin instead (because the hashtable can fit in the memory) and achieve better performance!</p>

<p>In our implementation, since we've build a hashtable beforehand, we now how many different keys the table contains (denoted as ICARD) and the total memory consumption of this hashtable (denoted as S). We can then get the selection factor sf by following equation:</p>

<pre><code>sf = 1 / ICARD
</code></pre>

<p>And we the the memory consumtion of this query's hashtable is:</p>

<pre><code>S' = S * sf
</code></pre>

<p>also we can get other predict equations of other kind of predicats.</p>

<p>If a hashtable with physical size S' can fit in mapper's memory, we use Map Join, otherwise, Common Join. This further increased the probabilities of using Map Join.</p>

<p>As a conclusion, current implementation of Hive losses many opportunities of using Map Join. We improved the chance a little bit by precomputing hash table in advance, and leverage the heuristic information when we execute a query.</p>
]]></content>
  </entry>
  
</feed>
